{"cells":[{"cell_type":"markdown","id":"62077047-aba2-4b81-b437-eb6f3bdf84d2","metadata":{},"outputs":[],"source":["\u003cp style=\"text-align:center\"\u003e\n","    \u003ca href=\"https://skills.network\" target=\"_blank\"\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"\u003e\n","    \u003c/a\u003e\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"c9806485-727d-45c3-baaa-5e41bc829d2c","metadata":{},"outputs":[],"source":["\u003ch1\u003ePrebuilt Datasets and Transforms\u003c/h1\u003e \n"]},{"cell_type":"markdown","id":"35a30e94-cf2e-4edf-bf90-7f70c20226ca","metadata":{},"outputs":[],"source":["\u003ch2\u003eObjective\u003c/h2\u003e\u003cul\u003e\u003cli\u003e How to use MNIST prebuilt dataset in pytorch.\u003c/li\u003e\u003c/ul\u003e \n"]},{"cell_type":"markdown","id":"8e793616-b42c-4594-98da-a25029b949a3","metadata":{},"outputs":[],"source":["\u003ch2\u003eTable of Contents\u003c/h2\u003e\n","\u003cp\u003eIn this lab, you will use a prebuilt dataset and then use some prebuilt dataset transforms.\u003c/p\u003e\n","\u003cul\u003e\n","    \u003cli\u003e\u003ca href=\"#Prebuilt_Dataset\"\u003ePrebuilt Datasets\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Torchvision\"\u003eTorchvision Transforms\u003c/a\u003e\u003c/li\u003e\n","\u003c/ul\u003e\n","\u003cp\u003eEstimated Time Needed: \u003cstrong\u003e10 min\u003c/strong\u003e\u003c/p\u003e\n","\n","\u003chr\u003e\n"]},{"cell_type":"markdown","id":"97131840-0727-4f6a-a001-90fdd28b16dc","metadata":{},"outputs":[],"source":["\u003ch2\u003ePreparation\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"0f7faec7-c8f5-4984-81a6-f4ad13c33af1","metadata":{},"outputs":[],"source":["The following are the libraries we are going to use for this lab. The \u003ccode\u003etorch.manual_seed()\u003c/code\u003e is for forcing the random function to give the same number every time we try to recompile it.\n"]},{"cell_type":"code","id":"56dc3f7d-7b6b-49a0-b1b4-3f5cbf999b33","metadata":{},"outputs":[],"source":["# These are the libraries will be used for this lab.\n\n!pip install torchvision==0.9.1 torch==1.8.1 \nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(0)"]},{"cell_type":"markdown","id":"11295c6a-1fa0-40b8-8169-8d4a96d46230","metadata":{},"outputs":[],"source":["This is the function for displaying images.\n"]},{"cell_type":"code","id":"1ecdc347-36e9-419d-9c4a-55b10f6c5f68","metadata":{},"outputs":[],"source":["# Show data by diagram\n\ndef show_data(data_sample, shape = (28, 28)):\n    plt.imshow(data_sample[0].numpy().reshape(shape), cmap='gray')\n    plt.title('y = ' + str(data_sample[1]))"]},{"cell_type":"markdown","id":"344253cf-79e1-4e1d-9dbb-02ebe685844f","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"56c2d02e-a436-4cbe-9616-24dc511aa9cb","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Prebuilt_Dataset\"\u003ePrebuilt Datasets\u003c/h2\u003e \n"]},{"cell_type":"markdown","id":"8eca52d2-cfd3-4c82-b986-6c38e7cfdaee","metadata":{},"outputs":[],"source":["You will focus on the following libraries: \n"]},{"cell_type":"code","id":"456b0149-3791-4cc1-a753-5c68bfdae566","metadata":{},"outputs":[],"source":["# Run the command below when you do not have torchvision installed\n# !mamba install -y torchvision\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets"]},{"cell_type":"markdown","id":"b4d4388a-a182-4358-bd9c-656dde75e49c","metadata":{},"outputs":[],"source":["We can import a prebuilt dataset. In this case, use MNIST. You'll work with several of these parameters later by placing a transform object in the argument \u003ccode\u003etransform\u003c/code\u003e.\n"]},{"cell_type":"code","id":"341f7366-1553-4f20-b4ec-c14ba8d69043","metadata":{},"outputs":[],"source":["# Import the prebuilt dataset into variable dataset\n\n\ndataset = dsets.MNIST(\n    root = './data',  \n    download = False, \n    transform = transforms.ToTensor()\n)"]},{"cell_type":"markdown","id":"bde2b97f-5620-40dc-93f4-666d1628a7a4","metadata":{},"outputs":[],"source":["Each element of the dataset object contains a tuple. Let us see whether the first element in the dataset is a tuple and what is in it.\n"]},{"cell_type":"code","id":"c43da1ca-59a0-42ba-985c-15cc969be3ad","metadata":{},"outputs":[],"source":["# Examine whether the elements in dataset MNIST are tuples, and what is in the tuple?\n\nprint(\"Type of the first element: \", type(dataset[0]))\nprint(\"The length of the tuple: \", len(dataset[0]))\nprint(\"The shape of the first element in the tuple: \", dataset[0][0].shape)\nprint(\"The type of the first element in the tuple\", type(dataset[0][0]))\nprint(\"The second element in the tuple: \", dataset[0][1])\nprint(\"The type of the second element in the tuple: \", type(dataset[0][1]))\nprint(\"As the result, the structure of the first element in the dataset is (tensor([1, 28, 28]), tensor(7)).\")"]},{"cell_type":"markdown","id":"8e89cc0f-33fb-44c5-8d0c-918edd82e4ad","metadata":{},"outputs":[],"source":["As shown in the output, the first element in the tuple is a cuboid tensor. As you can see, there is a dimension with only size 1, so basically, it is a rectangular tensor.\u003cbr\u003e\n","The second element in the tuple is a number tensor, which indicate the real number the image shows. As the second element in the tuple is \u003ccode\u003etensor(7)\u003c/code\u003e, the image should show a hand-written 7.\n"]},{"cell_type":"markdown","id":"04dd7f15-35c2-44a2-b76c-242ab64f3358","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"482ce1cd-52e9-4580-acf3-c970fbac971b","metadata":{},"outputs":[],"source":["Let us plot the first element in the dataset:\n"]},{"cell_type":"code","id":"4a01189d-3eb6-4a90-8fd3-b2492b8a9b3c","metadata":{},"outputs":[],"source":["# Plot the first element in the dataset\n\nshow_data(dataset[0])"]},{"cell_type":"markdown","id":"b2c105a1-eeb4-463c-ba75-c3b7d6cde072","metadata":{},"outputs":[],"source":["As we can see, it is a 7.\n"]},{"cell_type":"markdown","id":"42a29501-378f-476d-81f1-06ba6fc46ceb","metadata":{},"outputs":[],"source":["Plot the second sample:   \n"]},{"cell_type":"code","id":"eaca0dfa-ab3e-4c33-948a-195dd67ec7e9","metadata":{},"outputs":[],"source":["# Plot the second element in the dataset\n\nshow_data(dataset[1])"]},{"cell_type":"markdown","id":"bcb8a1f5-8a59-471c-b00f-64184ef7621e","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"103ce82f-efd4-4dc7-9088-431451a33b95","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Torchvision\"\u003e Torchvision Transforms  \u003c/h2\u003e \n"]},{"cell_type":"markdown","id":"406f0014-4769-43be-b1a6-4aba001af3fe","metadata":{},"outputs":[],"source":["We can apply some image transform functions on the MNIST dataset.\n"]},{"cell_type":"markdown","id":"20096baa-29bb-49e5-8e22-9987ef5cc62e","metadata":{},"outputs":[],"source":["As an example, the images in the MNIST dataset can be cropped and converted to a tensor. We can use \u003ccode\u003etransform.Compose\u003c/code\u003e we learned from the previous lab to combine the two transform functions.\n"]},{"cell_type":"code","id":"8668e87f-d10c-4950-81f2-df7deaa41090","metadata":{},"outputs":[],"source":["# Combine two transforms: crop and convert to tensor. Apply the compose to MNIST dataset\n\ncroptensor_data_transform = transforms.Compose([transforms.CenterCrop(20), transforms.ToTensor()])\ndataset = dsets.MNIST(root = './data', download = True, transform = croptensor_data_transform)\nprint(\"The shape of the first element in the first tuple: \", dataset[0][0].shape)"]},{"cell_type":"markdown","id":"030033c1-1cd9-4ec4-a9d6-bad6c63b2334","metadata":{},"outputs":[],"source":["We can see the image is now 20 x 20 instead of 28 x 28.\n"]},{"cell_type":"markdown","id":"02798f85-159c-4417-b29a-a958e8a795ec","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"a6616b22-fbdd-4d63-bbb6-5bd8be0d3ea9","metadata":{},"outputs":[],"source":["Let us plot the first image again. Notice that the black space around the \u003cb\u003e7\u003c/b\u003e become less apparent.\n"]},{"cell_type":"code","id":"8da841e2-e899-4477-a665-ef406e8dc914","metadata":{},"outputs":[],"source":["# Plot the first element in the dataset\n\nshow_data(dataset[0],shape = (20, 20))"]},{"cell_type":"code","id":"8de4f615-5eca-41c1-bd7b-35196e7a4e76","metadata":{},"outputs":[],"source":["# Plot the second element in the dataset\n\nshow_data(dataset[1],shape = (20, 20))"]},{"cell_type":"markdown","id":"390b3d8b-224b-47bf-af9d-b8ed52385964","metadata":{},"outputs":[],"source":["In the below example, we horizontally flip the image, and then convert it to a tensor. Use \u003ccode\u003etransforms.Compose()\u003c/code\u003e to combine these two transform functions. Plot the flipped image.\n"]},{"cell_type":"code","id":"419d46c2-78a5-4f7f-950b-557d938216de","metadata":{},"outputs":[],"source":["# Construct the compose. Apply it on MNIST dataset. Plot the image out.\n\nfliptensor_data_transform = transforms.Compose([transforms.RandomHorizontalFlip(p = 1),transforms.ToTensor()])\ndataset = dsets.MNIST(root = './data', download = True, transform = fliptensor_data_transform)\nshow_data(dataset[1])"]},{"cell_type":"markdown","id":"937195b6-004e-4a12-8682-17b1aa65a70e","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"650ae916-1c6e-40c0-a1f0-1ed5a7cbd81f","metadata":{},"outputs":[],"source":["\u003ch3\u003ePractice\u003c/h3\u003e\n"]},{"cell_type":"markdown","id":"1edcedf3-a8e5-42a5-bc36-76e3976f8785","metadata":{},"outputs":[],"source":["Try to use the \u003ccode\u003eRandomVerticalFlip\u003c/code\u003e (vertically flip the image) with horizontally flip and convert to tensor as a compose. Apply the compose on image. Use \u003ccode\u003eshow_data()\u003c/code\u003e to plot the second image (the image as \u003cb\u003e2\u003c/b\u003e).\n"]},{"cell_type":"code","id":"6bf42013-7fb2-4210-8f5d-ee8004eb527c","metadata":{},"outputs":[],"source":["# Practice: Combine vertical flip, horizontal flip and convert to tensor as a compose. Apply the compose on image. Then plot the image\n\n# Type your code here"]},{"cell_type":"markdown","id":"51cb7a9c-c70b-4994-a331-f10a2b18b39d","metadata":{},"outputs":[],"source":["Double-click **here** for the solution.\n","\n","\u003c!-- \n","my_data_transform = transforms.Compose([transforms.RandomVerticalFlip(p = 1), transforms.RandomHorizontalFlip(p = 1), transforms.ToTensor()])\n","dataset = dsets.MNIST(root = './data', train = False, download = True, transform = my_data_transform)\n","show_data(dataset[1])\n"," --\u003e\n"]},{"cell_type":"markdown","id":"0323969f-4d3d-42cc-9c1d-3dc3f19ae749","metadata":{},"outputs":[],"source":["\u003ca href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network\u0026utm_content=in_lab_content_link\u0026utm_id=Lab-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork\u0026context=cpdaas\u0026apps=data_science_experience%2Cwatson_machine_learning\"\u003e\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"\u003e\u003c/a\u003e\n"]},{"cell_type":"markdown","id":"18e3edda-7730-439f-9da4-b53f49dcccd8","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"bd6c672a-975b-4fe6-abc7-4b3cf809295c","metadata":{},"outputs":[],"source":["\u003ch2\u003eAbout the Authors:\u003c/h2\u003e \n","\n","\u003ca href=\"https://www.linkedin.com/in/joseph-s-50398b136/\"\u003eJoseph Santarcangelo\u003c/a\u003e has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"01740488-efc7-40d2-badf-e392f41cc782","metadata":{},"outputs":[],"source":["Other contributors: \u003ca href=\"https://www.linkedin.com/in/michelleccarey/\"\u003eMichelle Carey\u003c/a\u003e, \u003ca href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\"\u003eMavis Zhou\u003c/a\u003e \n"]},{"cell_type":"markdown","id":"487a01b4-e86e-40e0-a081-0d066a37943e","metadata":{},"outputs":[],"source":["\u003c!--\n","## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n","| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n","| 2020-09-21        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n","--\u003e\n"]},{"cell_type":"markdown","id":"acec5d7a-d8d2-443b-93cf-d984a2b52590","metadata":{},"outputs":[],"source":["\u003chr\u003e\n"]},{"cell_type":"markdown","id":"cd983475-74c1-464e-858e-2fade06ff1e3","metadata":{},"outputs":[],"source":["## \u003ch3 align=\"center\"\u003e \u0026#169; IBM Corporation. All rights reserved. \u003ch3/\u003e\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}