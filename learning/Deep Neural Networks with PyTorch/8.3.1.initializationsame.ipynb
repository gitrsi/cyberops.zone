{"cells":[{"cell_type":"markdown","id":"e87ffe8a-3689-411e-a417-2fa66fe0fab4","metadata":{},"outputs":[],"source":["\u003cp style=\"text-align:center\"\u003e\n","    \u003ca href=\"https://skills.network\" target=\"_blank\"\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"\u003e\n","    \u003c/a\u003e\n","\u003c/p\u003e\n"]},{"cell_type":"markdown","id":"7c8a8fd1-9cfc-4008-9f7f-528f955a58ae","metadata":{},"outputs":[],"source":["\u003ch1\u003eInitialization with Same Weights \u003c/h1\u003e\n"]},{"cell_type":"markdown","id":"c361fd12-3656-4a29-b16f-2394c44f74a3","metadata":{},"outputs":[],"source":["\n","\u003ch3\u003eObjective for this Notebook\u003ch3\u003e    \n","\u003ch5\u003e 1. Learn hw to Define the Neural Network with Same Weights Initialization define  Criterion Function, Optimizer, and Train the Model\u003c/h5\u003e\n","\u003ch5\u003e 2.Define the Neural Network with defult Weights Initialization define  Criterion Function, Optimizer\u003c/h5\u003e\n","\u003ch5\u003e 3. Train the Model \u003c/h5\u003e     \n","\n"]},{"cell_type":"markdown","id":"aedbce93-f1ac-4929-9d73-0e2fe3bfa6e8","metadata":{},"outputs":[],"source":["\u003ch2\u003eTable of Contents\u003c/h2\u003e\n","\u003cp\u003eIn this lab, we will see the problem of initializing the weights with the same value. We will see that even for a simple network, our model will not train properly. .\u003c/p\u003e\n","\n","\u003cul\u003e\n","    \u003cli\u003e\u003ca href=\"#Model\"\u003eNeural Network Module and Training Function\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Makeup_Data\"\u003eMake Some Data\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Train\"\u003eDefine the Neural Network with Same Weights Initialization define  Criterion Function, Optimizer, and Train the Model\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#Train\"\u003eDefine the Neural Network with defult Weights Initialization define  Criterion Function, Optimizer, and Train the Model\u003c/a\u003e\u003c/li\u003e\n","\u003c/ul\u003e\n","\u003cp\u003eEstimated Time Needed: \u003cstrong\u003e25 min\u003c/strong\u003e\u003c/p\u003e\n","\n","\u003chr\u003e\n"]},{"cell_type":"markdown","id":"0d091932-e7a7-4046-b127-4a61a3cd921d","metadata":{},"outputs":[],"source":["\u003ch2\u003ePreparation\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"5fde526e-58e6-48f4-81f5-70df75946d50","metadata":{},"outputs":[],"source":["We'll need the following libraries\n"]},{"cell_type":"code","id":"150ba60b-1027-4cb6-8436-dfdf851832f4","metadata":{},"outputs":[],"source":["# Import the libraries we need for this lab\n\nimport torch \nimport torch.nn as nn\nfrom torch import sigmoid\nimport matplotlib.pylab as plt\nimport numpy as np\ntorch.manual_seed(0)"]},{"cell_type":"markdown","id":"96cca366-8384-43ac-9e05-8c1a0fb27d3d","metadata":{},"outputs":[],"source":["Used for plotting the model\n"]},{"cell_type":"code","id":"9b2eb5d3-7b17-4f91-8e60-3ffe7fadd2cb","metadata":{},"outputs":[],"source":["# The function for plotting the model\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n    \n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    plt.xlabel('x')\n    if leg == True:\n        plt.legend()\n    else:\n        pass"]},{"cell_type":"markdown","id":"4d9b6602-8456-47f0-986c-6491fb01afb9","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"15ce89bc-b207-426a-a42c-58c6d5709736","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Model\"\u003eNeural Network Module and Training Function\u003c/h2\u003e \n"]},{"cell_type":"markdown","id":"d85bd6f9-3f38-40ff-b0ab-c59d19e9a8c1","metadata":{},"outputs":[],"source":["Define the activations and the output of the first linear layer as an attribute. Note that this is not good practice. \n"]},{"cell_type":"code","id":"b7a4167e-e50a-43dc-bc91-149beca36e86","metadata":{},"outputs":[],"source":["# Define the class Net\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        # hidden layer \n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n        # Define the first linear layer as an attribute, this is not good practice\n        self.a1 = None\n        self.l1 = None\n        self.l2=None\n    \n    # Prediction\n    def forward(self, x):\n        self.l1 = self.linear1(x)\n        self.a1 = sigmoid(self.l1)\n        self.l2=self.linear2(self.a1)\n        yhat = sigmoid(self.linear2(self.a1))\n        return yhat"]},{"cell_type":"markdown","id":"3bf701b3-17c8-4f8d-9898-d103fcd64679","metadata":{},"outputs":[],"source":["Define the training function:\n"]},{"cell_type":"code","id":"5c04a90b-a6c1-4f66-a0de-b91f75bb6a80","metadata":{},"outputs":[],"source":["# Define the training function\n\ndef train(Y, X, model, optimizer, criterion, epochs=1000):\n    cost = []\n    total=0\n    for epoch in range(epochs):\n        total=0\n        for y, x in zip(Y, X):\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            #cumulative loss \n            total+=loss.item() \n        cost.append(total)\n        if epoch % 300 == 0:    \n            PlotStuff(X, Y, model, epoch, leg=True)\n            plt.show()\n            model(X)\n            plt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1))\n            plt.title('activations')\n            plt.show()\n    return cost"]},{"cell_type":"markdown","id":"345bf20e-6f2f-4403-926d-97247625e178","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"c312df1e-b2d4-442b-afe4-4b553afb87b7","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Makeup_Data\"\u003eMake Some Data\u003c/h2\u003e\n"]},{"cell_type":"code","id":"f2a67ae1-91b2-4fb7-ad52-2b9912c7f64e","metadata":{},"outputs":[],"source":["# Make some data\n\nX = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor)\nY = torch.zeros(X.shape[0])\nY[(X[:, 0] \u003e -4) \u0026 (X[:, 0] \u003c 4)] = 1.0"]},{"cell_type":"markdown","id":"8fd08818-4c07-444b-8451-35d60b61320c","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"bedbeafc-7c09-43c6-bcea-70f58e44a4a0","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Train\"\u003eDefine the Neural Network with Same Weights Initialization define, Criterion Function, Optimizer and Train the Model\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"976b80b6-8812-4003-97e2-1a08ed50a5da","metadata":{},"outputs":[],"source":["Create the Cross-Entropy loss function: \n"]},{"cell_type":"code","id":"d9427dc1-7f71-4715-a4ba-b612afca2227","metadata":{},"outputs":[],"source":["# The loss function\n\ndef criterion_cross(outputs, labels):\n    out = -1 * torch.mean(labels * torch.log(outputs) + (1 - labels) * torch.log(1 - outputs))\n    return out"]},{"cell_type":"markdown","id":"bc4a21c3-c924-408e-aef1-a813ef72fe03","metadata":{},"outputs":[],"source":["Define the Neural Network\n"]},{"cell_type":"code","id":"a51948b2-2c0f-45dc-8faf-fd9f420a8273","metadata":{},"outputs":[],"source":["# Train the model\n# size of input \nD_in = 1\n# size of hidden layer \nH = 2\n# number of outputs \nD_out = 1\n# learning rate \nlearning_rate = 0.1\n# create the model \nmodel = Net(D_in, H, D_out)\n"]},{"cell_type":"markdown","id":"c04e9b4d-3c08-4fb3-84a4-478e26279c4a","metadata":{},"outputs":[],"source":["This is the PyTorch default installation\n"]},{"cell_type":"code","id":"185b9e53-1363-4d68-816d-6f54d16f5ce7","metadata":{},"outputs":[],"source":["model.state_dict()"]},{"cell_type":"markdown","id":"96e1a27e-a8ca-404b-b628-88550499a336","metadata":{},"outputs":[],"source":["Same Weights Initialization with all ones for weights and zeros for the bias.\n"]},{"cell_type":"code","id":"51018486-bd21-40c1-b456-438dfb9fde2b","metadata":{},"outputs":[],"source":["model.state_dict()['linear1.weight'][0]=1.0\nmodel.state_dict()['linear1.weight'][1]=1.0\nmodel.state_dict()['linear1.bias'][0]=0.0\nmodel.state_dict()['linear1.bias'][1]=0.0\nmodel.state_dict()['linear2.weight'][0]=1.0\nmodel.state_dict()['linear2.bias'][0]=0.0\nmodel.state_dict()"]},{"cell_type":"markdown","id":"dd55deb4-f25e-49f0-8dd3-5f1829b91323","metadata":{},"outputs":[],"source":["Optimizer, and Train the Model:\n"]},{"cell_type":"code","id":"929d7788-0c58-41a2-9a74-473b0c495aa1","metadata":{},"outputs":[],"source":["#optimizer \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n#train the model usein\ncost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)\n#plot the loss\nplt.plot(cost_cross)\nplt.xlabel('epoch')\nplt.title('cross entropy loss')"]},{"cell_type":"markdown","id":"94432e06-9bdd-44c1-be6b-bad12970517e","metadata":{},"outputs":[],"source":["By examining the output of the  paramters all thought they have changed they are identical.\n"]},{"cell_type":"code","id":"d6c6002d-af75-4da7-8b73-da76d3d9af02","metadata":{},"outputs":[],"source":["model.state_dict()"]},{"cell_type":"code","id":"52bc28c9-a7f2-46e8-9a9d-78a3575bb7d5","metadata":{},"outputs":[],"source":["yhat=model(torch.tensor([[-2.0],[0.0],[2.0]]))\nyhat"]},{"cell_type":"markdown","id":"e72986b4-fa07-4183-b7ef-b0e8e84765ae","metadata":{},"outputs":[],"source":["\u003ch2 id=\"Train2\"\u003eDefine the Neural Network, Criterion Function, Optimizer and Train the Model \u003c/h2\u003e\n"]},{"cell_type":"code","id":"653c1277-2187-41e0-9e84-ef26805b232e","metadata":{},"outputs":[],"source":["# Train the model\n# size of input \nD_in = 1\n# size of hidden layer \nH = 2\n# number of outputs \nD_out = 1\n# learning rate \nlearning_rate = 0.1\n# create the model \nmodel = Net(D_in, H, D_out)"]},{"cell_type":"markdown","id":"10596304-6354-4ab1-87f3-bca100859c03","metadata":{},"outputs":[],"source":["Repeat the previous steps above by using the MSE cost or total loss: \n"]},{"cell_type":"code","id":"e795e5a4-de63-45f9-83cf-2ef1cf80b6d4","metadata":{},"outputs":[],"source":["#optimizer \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n#train the model usein\ncost_cross = train(Y, X, model, optimizer, criterion_cross, epochs=1000)\n#plot the loss\nplt.plot(cost_cross)\nplt.xlabel('epoch')\nplt.title('cross entropy loss')"]},{"cell_type":"markdown","id":"798e795c-1351-49f8-9fd7-566dc3224fe2","metadata":{},"outputs":[],"source":["Double-click \u003cb\u003ehere\u003c/b\u003e for the solution.\n","\n","\u003c!-- \n","learning_rate = 0.1\n","criterion_mse=nn.MSELoss()\n","model=Net(D_in,H,D_out)\n","optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)\n","cost_mse=train(Y,X,model,optimizer,criterion_mse,epochs=1000)\n","plt.plot(cost_mse)\n","plt.xlabel('epoch')\n","plt.title('MSE loss ')\n","--\u003e\n"]},{"cell_type":"markdown","id":"92bb1ad5-03f3-4e87-9eb3-f802b93b23f8","metadata":{},"outputs":[],"source":["\n","\u003ca href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network\u0026utm_content=in_lab_content_link\u0026utm_id=Lab-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork\u0026context=cpdaas\u0026apps=data_science_experience%2Cwatson_machine_learning\"\u003e\u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"\u003e\u003c/a\u003e\n"]},{"cell_type":"markdown","id":"f617f666-b9a3-4d50-a4fa-5107fef2ef63","metadata":{},"outputs":[],"source":["\u003c!--Empty Space for separating topics--\u003e\n"]},{"cell_type":"markdown","id":"13e7f6ec-bb2e-4a67-9c03-0f4732ade64d","metadata":{},"outputs":[],"source":["\u003ch2\u003eAbout the Authors:\u003c/h2\u003e \n","\n","\u003ca href=\"https://www.linkedin.com/in/joseph-s-50398b136/\"\u003eJoseph Santarcangelo\u003c/a\u003e has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD. \n"]},{"cell_type":"markdown","id":"c4290a23-fdfb-493f-a223-9053818f3235","metadata":{},"outputs":[],"source":["Other contributors: \u003ca href=\"https://www.linkedin.com/in/michelleccarey/\"\u003eMichelle Carey\u003c/a\u003e, \u003ca href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\"\u003eMavis Zhou\u003c/a\u003e\n"]},{"cell_type":"markdown","id":"a27d3c54-32af-4e24-867c-f72abb8f262c","metadata":{},"outputs":[],"source":["\u003c!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","\n","\n","\u003chr\u003e\n","--\u003e\n","\n","## \u003ch3 align=\"center\"\u003e \u0026#169; IBM Corporation. All rights reserved. \u003ch3/\u003e\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}