{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec3b2437f174ffaa6c059ad349c46d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea21da044e8947e888db95439e2b3aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x7f1aee069850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-28 08:00:54.688566: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2024-03-28 08:00:54.692668: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394320000 Hz\n",
      "2024-03-28 08:00:54.693300: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b60b69e540 executing computations on platform Host. Devices:\n",
      "2024-03-28 08:00:54.693345: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2024-03-28 08:00:54.718327: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7f1a671e5990>,\n",
       " <keras.layers.core.Dense at 0x7f1a6504c810>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f1aee01a150>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7f1aee07a1d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1aee07acd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1aee06d810>,\n",
       " <keras.layers.core.Activation at 0x7f1b3034d550>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f1aee09b190>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1aede63d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1aedeb9850>,\n",
       " <keras.layers.core.Activation at 0x7f1aeddc3850>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1aedd53590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1aedd9d090>,\n",
       " <keras.layers.core.Activation at 0x7f1aedd16290>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1aec05c7d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ae4726e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ae4788ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ae46dc090>,\n",
       " <keras.layers.merge.Add at 0x7f1ae46dc9d0>,\n",
       " <keras.layers.core.Activation at 0x7f1ae4629c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ae45dc7d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ae45a6c90>,\n",
       " <keras.layers.core.Activation at 0x7f1ae45a6fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ae44e3c90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ae44b73d0>,\n",
       " <keras.layers.core.Activation at 0x7f1ae44b71d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ae43d2690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ae434df10>,\n",
       " <keras.layers.merge.Add at 0x7f1ae4368210>,\n",
       " <keras.layers.core.Activation at 0x7f1ae42eebd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ae429e990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ae4267a50>,\n",
       " <keras.layers.core.Activation at 0x7f1ae4229f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ae41e7f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ae4161210>,\n",
       " <keras.layers.core.Activation at 0x7f1ae41612d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ae407d850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ac07d3150>,\n",
       " <keras.layers.merge.Add at 0x7f1ae4065b90>,\n",
       " <keras.layers.core.Activation at 0x7f1ac075a410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ac075a450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ac06bbf50>,\n",
       " <keras.layers.core.Activation at 0x7f1ac06bb550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ac0651ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ac05cc190>,\n",
       " <keras.layers.core.Activation at 0x7f1ac05cc250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ac0541510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ac0459ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ac04dfa50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ac0476690>,\n",
       " <keras.layers.merge.Add at 0x7f1ac03404d0>,\n",
       " <keras.layers.core.Activation at 0x7f1ac0354ed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ac02ef090>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ac02bfa90>,\n",
       " <keras.layers.core.Activation at 0x7f1ac02d4d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ac0272ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ac01d9990>,\n",
       " <keras.layers.core.Activation at 0x7f1ac01d9fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1ac0170cd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1ac00e8110>,\n",
       " <keras.layers.merge.Add at 0x7f1ac00e81d0>,\n",
       " <keras.layers.core.Activation at 0x7f1aa87c6750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1aa87eac50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1aa8740b10>,\n",
       " <keras.layers.core.Activation at 0x7f1aa8740f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1aa86dfe50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1aa866b190>,\n",
       " <keras.layers.core.Activation at 0x7f1aa8643610>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1aa85dde50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1aa8542b50>,\n",
       " <keras.layers.merge.Add at 0x7f1aa8542f50>,\n",
       " <keras.layers.core.Activation at 0x7f1aa84f4210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1aa86df590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1aa8458a10>,\n",
       " <keras.layers.core.Activation at 0x7f1aa846d8d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1aa838dc90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1aa8327450>,\n",
       " <keras.layers.core.Activation at 0x7f1aa84f46d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1aa8288650>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1aa81cea50>,\n",
       " <keras.layers.merge.Add at 0x7f1aa8208590>,\n",
       " <keras.layers.core.Activation at 0x7f1aa819e250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1aa80e2f50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1aa8116850>,\n",
       " <keras.layers.core.Activation at 0x7f1aa8116fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1aa803cfd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a84f97390>,\n",
       " <keras.layers.core.Activation at 0x7f1a84fd9f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a84f72e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a84e0d590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a84e95290>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a84e2e8d0>,\n",
       " <keras.layers.merge.Add at 0x7f1a84dc1b50>,\n",
       " <keras.layers.core.Activation at 0x7f1a84d14c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a84cb6590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a84c8ffd0>,\n",
       " <keras.layers.core.Activation at 0x7f1a84c8f110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a84bbd910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a84b5ad90>,\n",
       " <keras.layers.core.Activation at 0x7f1a84ba19d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a84ac0c10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a84ab6310>,\n",
       " <keras.layers.merge.Add at 0x7f1a84ab6450>,\n",
       " <keras.layers.core.Activation at 0x7f1a849d1710>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a849d1150>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a8494ded0>,\n",
       " <keras.layers.core.Activation at 0x7f1a8494da90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a848e7ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a84850b50>,\n",
       " <keras.layers.core.Activation at 0x7f1a84850f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a847820d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a8471e7d0>,\n",
       " <keras.layers.merge.Add at 0x7f1a847639d0>,\n",
       " <keras.layers.core.Activation at 0x7f1a84683c50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a847826d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a846796d0>,\n",
       " <keras.layers.core.Activation at 0x7f1a84679c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a845b2d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a844bb1d0>,\n",
       " <keras.layers.core.Activation at 0x7f1a84514650>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a844acb10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a843f4b50>,\n",
       " <keras.layers.merge.Add at 0x7f1a844258d0>,\n",
       " <keras.layers.core.Activation at 0x7f1a84344250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a845b2cd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a84327a50>,\n",
       " <keras.layers.core.Activation at 0x7f1a842baf10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a84257ad0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a841c0e50>,\n",
       " <keras.layers.core.Activation at 0x7f1a841c0d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a84171d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a840dae50>,\n",
       " <keras.layers.merge.Add at 0x7f1a840daf10>,\n",
       " <keras.layers.core.Activation at 0x7f1a8406fcd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a67fda890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a67f45ed0>,\n",
       " <keras.layers.core.Activation at 0x7f1a84171cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a67ef1a50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a67e45450>,\n",
       " <keras.layers.core.Activation at 0x7f1a67e45fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a67de0210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a67d49fd0>,\n",
       " <keras.layers.merge.Add at 0x7f1a67d5eb90>,\n",
       " <keras.layers.core.Activation at 0x7f1a67cf8d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a67c97750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a67c5ff90>,\n",
       " <keras.layers.core.Activation at 0x7f1a67c75d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a67bb2990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a67b73f50>,\n",
       " <keras.layers.core.Activation at 0x7f1a67a93950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a67aa9dd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a679a5a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a67a0a610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a6795c0d0>,\n",
       " <keras.layers.merge.Add at 0x7f1a678a7550>,\n",
       " <keras.layers.core.Activation at 0x7f1a67864cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a678bbcd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a6783af90>,\n",
       " <keras.layers.core.Activation at 0x7f1a6783ad50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a67756050>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a676c2dd0>,\n",
       " <keras.layers.core.Activation at 0x7f1a67657c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a6766de90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a675cfa50>,\n",
       " <keras.layers.merge.Add at 0x7f1a675cfad0>,\n",
       " <keras.layers.core.Activation at 0x7f1a6756ec10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a67523410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a674e8310>,\n",
       " <keras.layers.core.Activation at 0x7f1a674e8410>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a6741ec90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a673b6390>,\n",
       " <keras.layers.core.Activation at 0x7f1a673ff110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f1a67318350>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7f1a672b9250>,\n",
       " <keras.layers.merge.Add at 0x7f1a672970d0>,\n",
       " <keras.layers.core.Activation at 0x7f1a672321d0>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f1a6756e890>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7f1aede63c50>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/1\n",
      "101/101 [==============================] - 9729s 96s/step - loss: 0.0935 - acc: 0.9662 - val_loss: 0.2847 - val_acc: 0.8634\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs-1,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model_1_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 98/101 [============================>.] - ETA: 3:07 - loss: 0.0247 - acc: 0.9950"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
